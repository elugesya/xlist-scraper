# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

X List Scraper: A Twitter/X List scraper with dual interfaces (CLI + HTTP API). Uses Playwright for browser automation, Fastify for the API server, and Zod for schema validation. Exports clean JSON data with full engagement metrics.

## Build & Development Commands

```bash
# Install dependencies (requires pnpm)
pnpm install

# Build TypeScript to dist/
pnpm build

# Development server with hot reload
pnpm dev

# Production API server
pnpm start
# Or: node dist/api/server.js

# Testing
pnpm test                    # Run all tests once
pnpm test:watch              # Watch mode
pnpm test -- --coverage      # With coverage

# Code quality
pnpm lint                    # ESLint
pnpm format                  # Prettier

# CLI usage (after build)
node dist/cli.js "https://x.com/i/lists/123"
# Or use the bin wrapper:
./bin/xlist-scrape "https://x.com/i/lists/123" --max-tweets 500
```

## Architecture & Data Flow

### Three Entry Points

1. **CLI** (`src/cli.ts`) → `scrapeList()` → stdout JSON
2. **API** (`src/api/server.ts`) → routes → `scrapeList()` → HTTP response
3. **Library** (`src/index.ts` exports) → direct function imports

### Core Scraping Flow

`scrapeList()` in `src/scraper/scrapeList.ts`:
1. Launch Playwright Chromium browser (with optional proxy/cookies)
2. Navigate to list URL and check for blockers (login wall, rate limits)
3. Infinite scroll loop: parse tweet `<article>` elements via `domParsers.ts`
4. Dedupe by tweet ID, continue until `maxTweets` or end-of-feed detected
5. Sort by `createdAt` (newest first) and return

Tweet parsing (`src/scraper/domParsers.ts`):
- Extract ID from `/status/` link
- Parse engagement counts (reply/retweet/like) from button elements
- Use utility functions from `src/lib/` to normalize counts (1.2K → 1200) and format timestamps (Twitter's `EEE MMM dd HH:mm:ss +0000 yyyy` format)
- Detect retweets/quotes via DOM indicators

### API Request Handling

`POST /scrape/list` in `src/api/routes.ts`:
1. Validate request body with Zod schemas (`src/api/schemas.ts`)
2. **Key behavior**: Accept both hyphenated (`max-tweets`) and camelCase (`maxTweets`) keys; hyphenated takes precedence (see `normalizeRequest()`)
3. Use `p-limit` for controlled concurrency (default: sequential, set via `CONCURRENCY` env)
4. Add `sourceListURL` field to each tweet for multi-list requests
5. Merge results from all lists, sort globally by `createdAt`
6. Return structured responses: 200 (success), 422 (partial), 429 (rate limit), 400 (validation), 500 (error)

### Utility Libraries

- `src/lib/counts.ts`: Parse "1.2K", "3M", "5B" → numbers
- `src/lib/time.ts`: Format Date → Twitter timestamp string, parse relative times
- `src/lib/url.ts`: Validate list URLs, extract IDs, normalize twitter.com → x.com

### Type System

Core types in `src/types.ts`:
- `Tweet`: Base scraper output (11 fields including engagement counts)
- `ApiTweet`: Extends Tweet with `sourceListURL` for API responses
- `ScrapeOptions`: All scraper configuration (maxTweets, timeoutMs, headless, cookies, proxy, partialOk)

## Critical Implementation Details

### Parameter Naming Convention
The API accepts **both** hyphenated and camelCase keys for n8n compatibility:
- `max-tweets` / `maxTweets`
- `timeout-ms` / `timeoutMs`
- `listURL` / `listUrl`

**Hyphenated keys always win** if both provided (see `normalizeRequest()` in `schemas.ts`).

### Cookie Persistence
For authenticated scraping (avoid rate limits):
- Load cookies from `persistCookiesPath` (Playwright `storageState`)
- Save cookies back after scraping
- Format: Playwright's cookie array JSON

### Concurrency Control
API uses `p-limit` to control parallel scraping:
- Default: `CONCURRENCY=1` (sequential to avoid detection)
- Each list gets its own browser instance
- Results merged and sorted by timestamp after all complete

### Error Handling Strategy
- `LOGIN_REQUIRED`: Thrown when login wall detected → 429 response
- `RATE_LIMIT`: Thrown on rate limit detection → 429 response
- Validation errors: Zod errors → 400 with error details
- Partial results: If some lists fail but others succeed → 422 with both items and error details

### Timestamp Format
All `createdAt` fields use Twitter's legacy format:
```
"Tue Nov 04 19:06:32 +0000 2025"
```
Generated by `formatTwitterTimestamp()` in `src/lib/time.ts`. Always UTC.

### Testing Strategy
- Unit tests for pure functions (counts, time, URL parsing) in `*.test.ts` files
- Schema validation tests for Zod schemas and normalization logic
- API integration tests using Fastify's `inject()` method (no real scraping)
- All tests use Vitest with `vitest.config.ts`

## Environment Configuration

Key environment variables (see `.env.example`):
- `PORT`: API server port (default: 8080)
- `HEADLESS`: Browser mode (default: true)
- `CONCURRENCY`: Parallel scrapes (default: 1, keep low!)
- `PERSIST_COOKIES_PATH`: Cookie file location for authentication
- `PROXY`: HTTP proxy string
- `AUTH_MODE`: off/optional/required (auth middleware disabled by default)

## Deployment Notes

- **Docker**: Uses `mcr.microsoft.com/playwright:v1.42.1-jammy` base image
- **Init script**: `init.sh` automates full setup on Linux VMs
- **PM2**: Ecosystem file included for production process management
- **Health endpoint**: `GET /health` for monitoring
- **OpenAPI**: Auto-generated at `/docs` via `@fastify/swagger`

## Known Limitations

- **quoteCount**: Often 0 (not reliably in DOM without interaction)
- **bookmarkCount**: Always 0 (requires authentication + interaction)
- Private lists require authenticated cookies
- High concurrency may trigger Twitter rate limits/detection
- DOM selectors may break if Twitter changes their markup (look for `data-testid` attributes)

## When Modifying Scraping Logic

1. Test with real Twitter lists (not just unit tests)
2. Update DOM selectors in `domParsers.ts` if Twitter changes markup
3. Maintain the infinite scroll pattern with `consecutiveNoNewTweets` counter
4. Always cleanup browser resources in finally blocks
5. Log to stderr in CLI mode, use Fastify logger in API mode
